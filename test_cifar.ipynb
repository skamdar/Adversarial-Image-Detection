{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "#import foolbox\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms, utils\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import argparse\n",
    "from numpy import ma\n",
    "import scipy\n",
    "import os.path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "from skimage import feature\n",
    "from skimage import color\n",
    "from skimage import io\n",
    "\n",
    "from network.cifar10_cnn_net import cifar10_cnn_net\n",
    "from network.cifar10_ann import ann_net\n",
    "from network.adv_cifar10_cnn_net import adv_cifar10_cnn_net\n",
    "from network.adv_cifar10_ann import adv_ann_net\n",
    "import utils.cw_final as cw_final\n",
    "import utils.cw as cw\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "\n",
    "def train_binary(args, model, device, train_loader, optimizer, epoch, target_class):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        batch_size = target.numpy().shape[0]\n",
    "        \n",
    "        ## canny edge detection stub\n",
    "        #print(data.size())\n",
    "        edge_data = torch.zeros(batch_size, 1024)\n",
    "        for i in range(batch_size):\n",
    "            img = color.rgb2gray(data[i].permute(1,2,0).numpy())\n",
    "            edge_data[i] = torch.Tensor(feature.canny(img, sigma=1.8).astype(float)).reshape(1024)\n",
    "        \n",
    "        #data = data.reshape(-1, 784) # Remove this line while training cnn\n",
    "        edge_data, target = edge_data.to(device), target.to(device)\n",
    "        \n",
    "        #print(size)\n",
    "        for i in range(batch_size):\n",
    "            if target[i] != target_class:\n",
    "                #print(target[i])\n",
    "                target[i] = 1\n",
    "                #print(target[i])\n",
    "            else:\n",
    "                target[i] = 0\n",
    "        optimizer.zero_grad()\n",
    "        output = model(edge_data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    \n",
    "def test_binary(args, model, device, test_loader, target_class):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            batch_size = data.size(0)\n",
    "            #canny edge detection stub\n",
    "            edge_data = torch.zeros(batch_size, 1024)\n",
    "            for i in range(batch_size):\n",
    "                img = color.rgb2gray(data[i].permute(1,2,0).numpy())\n",
    "                edge_data[i] = torch.Tensor(feature.canny(img, sigma=1.8).astype(float)).reshape(1024)\n",
    "                if target[i].data != target_class:\n",
    "                    target[i] = torch.Tensor([1]).type(torch.LongTensor)#.to(torch.LongTensor)\n",
    "                else:\n",
    "                    target[i] = torch.Tensor([0]).type(torch.LongTensor)\n",
    "            \n",
    "            #data = data.reshape(data.size(0), 784) # Remove this line while training cnn\n",
    "                \n",
    "            edge_data, target = edge_data.to(device), target.to(device)\n",
    "            output = model(edge_data)\n",
    "            #print(output)\n",
    "            #test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1)#, keepdim=True)  # get the index of the max log-probability            \n",
    "            #print(pred[0])\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=100, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=50, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=True,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--save-model', action='store_true', default=True,\n",
    "                    help='For Saving the current Model')\n",
    "args = parser.parse_args([])\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available() \n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('data/cifar10', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0,), (1,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('data/cifar10', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0,), (1,))\n",
    "    ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cifar10_cnn_net().to(device)\n",
    "if (args.save_model):\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            train(args, model, device, train_loader, optimizer, epoch)\n",
    "            test(args, model, device, test_loader)\n",
    "\n",
    "        torch.save(model.state_dict(), \"cifar_cnn.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Model Accuracy:\n",
    "\n",
    "Test set: Average loss: 0.5847, Accuracy: 8163/10000 (82%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train binary classifiers\n",
    "\n",
    "model = ann_net().to(device)\n",
    "optimizer = None\n",
    "success = 0\n",
    "for m in range(10):\n",
    "    # for training of the cnn-network\n",
    "    if (args.save_model):\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            train_binary(args, model, device, train_loader, optimizer, epoch, m)\n",
    "            test_binary(args, model, device, test_loader, m)\n",
    "\n",
    "        torch.save(model.state_dict(), \"models/cifar_ann_canny/cifar_ann_canny_binary_sigma_1.8_\"+ str(m) + \".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accruacy for binary Classifier 0 \n",
    "Accruacy for binary Classifier 1 \n",
    "Accruacy for binary Classifier 2 \n",
    "Accruacy for binary Classifier 3 \n",
    "Accruacy for binary Classifier 4 \n",
    "Accruacy for binary Classifier 5 \n",
    "Accruacy for binary Classifier 6 \n",
    "Accruacy for binary Classifier 7 \n",
    "Accruacy for binary Classifier 8 \n",
    "Accruacy for binary Classifier 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UD case: \n",
    "#       1) create adv imgs for trained model without defence for different confidence values\n",
    "#       2) Mix with equal number of natural imgs\n",
    "#       3) Calculate accuracy with defence\n",
    "conf_list = [0, 10, 20]\n",
    "for conf in conf_list:\n",
    "    model = adv_cifar10_cnn_net()#.to(device)\n",
    "    model.load_state_dict(torch.load(\"cifar_cnn.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    classifier = cifar10_cnn_net()\n",
    "    classifier.load_state_dict(torch.load(\"cifar_cnn.pt\"))\n",
    "    classifier.eval()\n",
    "\n",
    "    d0 = ann_net()\n",
    "    d1 = ann_net()\n",
    "    d2 = ann_net()\n",
    "    d3 = ann_net()\n",
    "    d4 = ann_net()\n",
    "    d5 = ann_net()\n",
    "    d6 = ann_net()\n",
    "    d7 = ann_net()\n",
    "    d8 = ann_net()\n",
    "    d9 = ann_net()\n",
    "    detectors = [d0, d1,d2,d3,d4,d5,d6,d7,d8,d9]#.to(device)\n",
    "\n",
    "    for i in range(10):\n",
    "        detectors[i].load_state_dict(torch.load(\"models/cifar_ann_canny/mnist_ann_canny_binary_sigma_1.8_\"+\\\n",
    "                                                str(i) + \".pt\"))\n",
    "        detectors[i].eval()\n",
    "\n",
    "    inputs_box = (0.0, 1.0)\n",
    "    adversary = cw.L2Adversary(targeted=False,\n",
    "                                   confidence=conf,\n",
    "                                   search_steps=9,\n",
    "                                   box=inputs_box,\n",
    "                                   optimizer_lr=0.01)\n",
    "    adv_imgs = []\n",
    "    counter = 0\n",
    "    \n",
    "    for data, target in test_loader:\n",
    "        adv, l2 = adversary(model, data, target, to_numpy=False)\n",
    "        for i in range(args.test_batch_size):\n",
    "            if l2[i] != np.inf:    #check if attack was successful\n",
    "                adv_imgs.append(adv[i].reshape(32, 32).numpy())\n",
    "        if counter == 10:\n",
    "            break\n",
    "        else:\n",
    "            counter += 1\n",
    "            \n",
    "    with open(\"adv_imgs/cifar/UD_cifar_adv_imgs_cw_conf_\"+str(conf)+\".pkl\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(adv_imgs, fp)            \n",
    "    \n",
    "    print(\"UD Case\")\n",
    "    print(\"Confidence: \", conf)\n",
    "    print(\"Attack Success Rate: \", len(adv_imgs) / 1000)\n",
    "    print(\"Avg l2: \", np.mean(l2))\n",
    "    \n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    true_neg = 0\n",
    "    false_neg = 0\n",
    "\n",
    "    for i in range(len(adv_imgs)):\n",
    "            adv_im = adv_imgs[i]\n",
    "\n",
    "            # canny edge sdetection stub\n",
    "            #print(adv_im.shape)\n",
    "            #sigma = 1.8 gives best results\n",
    "\n",
    "            output = classifier(torch.Tensor(adv_im).reshape(1, 3, 32, 32))\n",
    "            #print(output)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            #print(pred)\n",
    "            adv_im = color.rgb2gray(data[i].permute(1,2,0).numpy())\n",
    "            adv_edge = feature.canny(adv_im, sigma=1.8).astype(float)\n",
    "            det_out = detectors[pred](torch.Tensor(adv_edge).reshape(1, 1024))\n",
    "            prediction = det_out.argmax(dim=1, keepdim=True)\n",
    "            if prediction == 0:\n",
    "                false_pos += 1\n",
    "            else:\n",
    "                true_pos += 1\n",
    "        #print(m)\n",
    "        #print(success1/len(train_adv_imgs[m]))\n",
    "    counter = 0\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = classifier(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        for i in range(args.test_batch_size):\n",
    "            prediction = np.asscalar(pred[i].numpy())\n",
    "            #detector.load_state_dict(torch.load(\"models/mnist_ann_canny/mnist_ann_canny_binary\"+ str(pred) + \".pt\"))\n",
    "            #detector.eval()\n",
    "            if (prediction != np.asscalar(target[i].numpy())): #ignoring natural errors\n",
    "                continue \n",
    "\n",
    "            img = color.rgb2gray(data[i].permute(1,2,0).numpy())\n",
    "            edge_data = feature.canny(img, sigma=1.8).astype(float)\n",
    "            output = detectors[prediction](torch.Tensor(edge_data).reshape(1, 1024))\n",
    "            prediction = output.argmax(dim=1, keepdim=True)\n",
    "            if prediction == 0:\n",
    "                true_neg += 1\n",
    "            else:\n",
    "                false_neg += 1\n",
    "        if counter == 10:\n",
    "            break\n",
    "        else:\n",
    "            counter += 1\n",
    "    print(\"True Positive: \", true_pos)\n",
    "    print(\"False Positive: \", false_pos)\n",
    "    print(\"True Negative: \", true_neg)\n",
    "    print(\"False Negative: \", false_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KD Case: Targeted\n",
    "conf_list = [0, 10, 20]\n",
    "for conf in conf_list:\n",
    "    model = cnn_net()#.to(device)\n",
    "    model.load_state_dict(torch.load(\"cifar_cnn.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    inputs_box = (0.0, 1.0)\n",
    "    adversary = cw_final.L2Adversary(targeted=True,\n",
    "                                   confidence=conf,\n",
    "                                   search_steps=20,\n",
    "                                   box=inputs_box,\n",
    "                                   optimizer_lr=0.005)\n",
    "\n",
    "    adv_imgs = [[] for i in range(10)]\n",
    "    l2_norms = [[] for i in range(10)]\n",
    "    success = 0\n",
    "    class_counter = 0    \n",
    "\n",
    "    for target_class in range(10):\n",
    "\n",
    "        detector = adv_ann_net()#.to(device)\n",
    "        detector.load_state_dict(torch.load(\"models/cifar_ann_canny/cifar_ann_canny_binary_sigma_1.8_\"+\\\n",
    "                                            str(target_class)+\".pt\"))\n",
    "        detector.eval()\n",
    "\n",
    "        # re define test-loader with batch_size 100\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "                    datasets.CIFAR10('data/cifar10', train=False, transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0,), (1,))])),\n",
    "                    batch_size=100, shuffle=True, **kwargs)\n",
    "        \n",
    "        counter = 0\n",
    "        for data, label in test_loader:\n",
    "            #data = torch.Tensor(feature.canny(data.reshape(28, 28).numpy(), sigma=1.0).astype(float)).reshape(1,1,28,28)\n",
    "            #print(label)\n",
    "            #if np.asscalar(target.numpy()) == 0:\n",
    "            #   continue\n",
    "            target = torch.Tensor([target_class]*100).type(torch.LongTensor)#.to(torch.LongTensor)\n",
    "            adv, batch_l2_norm = adversary(model, detector, data, target, to_numpy=False)\n",
    "\n",
    "\n",
    "            #assert isinstance(adversarial_examples, torch.FloatTensor)\n",
    "            #assert adv.size() == inputs.size()\n",
    "            #print(success)\n",
    "            for i in range(100):\n",
    "                #print(adv.size())\n",
    "                if batch_l2_norm[i] != np.inf and label[i] != target_class:\n",
    "                    #output = model(adv[i].reshape(1,1,28,28))\n",
    "                    #pred = output.argmax(dim=1, keepdim=True)\n",
    "                    #print(\"out \", pred)\n",
    "                    #if np.asscalar(pred.numpy()) == np.asscalar(target.numpy()[i]):\n",
    "                    #    print(\"appended\")\n",
    "                    success += 1\n",
    "                    adv_imgs[target_class].append(adv[i])\n",
    "                    l2_norms[target_class].append(batch_l2_norm[i])\n",
    "                if label[i] == target_class:\n",
    "                    class_counter += 1\n",
    "            if counter == 11:\n",
    "                break\n",
    "            else:\n",
    "                counter += 1\n",
    "            \n",
    "    print(\"KD Case, conf\")\n",
    "    print(\"success rate: \", success/(1100 - class_counter))\n",
    "    print(\"Examples processed: \", 1100 - class_counter)\n",
    "    l = []\n",
    "    for i in range(10):\n",
    "        l = l + l2_norms[i]\n",
    "    print(\"mean l2 norm:\", np.mean(l))\n",
    "\n",
    "        '''with open(\"adv_imgs/cifar/cw_cifar_adv_imgs_conf_\"+str(conf)+\"_test_loader_KD_targeted_sigma_1.8_\"+\\\n",
    "                      str(target_class)+\".pkl\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(adv_imgs, fp)\n",
    "        with open(\"adv_imgs/cifar/cw_cifar_l2_norms_conf_\"+str(conf)+\"_test_loader_KD_targeted_sigma_1.8_\"+\\\n",
    "                      str(target_class)+\".pkl\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(adv_imgs, fp) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KD Case: Untargeted\n",
    "\n",
    "conf_list = [0, 10, 20]\n",
    "for conf in conf_list:\n",
    "    model = cnn_net()#.to(device)\n",
    "    model.load_state_dict(torch.load(\"cifar_cnn.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    d0 = ann_net()\n",
    "    d1 = ann_net()\n",
    "    d2 = ann_net()\n",
    "    d3 = ann_net()\n",
    "    d4 = ann_net()\n",
    "    d5 = ann_net()\n",
    "    d6 = ann_net()\n",
    "    d7 = ann_net()\n",
    "    d8 = ann_net()\n",
    "    d9 = ann_net()\n",
    "    detectors = [d0, d1,d2,d3,d4,d5,d6,d7,d8,d9]#.to(device)\n",
    "\n",
    "    for i in range(10):\n",
    "        detectors[i].load_state_dict(torch.load(\"models/cifar_ann_canny/cifar_ann_canny_binary_sigma_1.8_\"+ str(i) + \".pt\"))\n",
    "        detectors[i].eval()\n",
    "\n",
    "    inputs_box = (0.0, 1.0)\n",
    "    adversary = cw_final.L2Adversary(targeted=False,\n",
    "                                     confidence=conf,\n",
    "                                     search_steps=9,\n",
    "                                     box=inputs_box,\n",
    "                                     optimizer_lr=0.01)\n",
    "\n",
    "    adv_imgs = []\n",
    "    l2_norms = []\n",
    "    success = 0\n",
    "    counter = 0\n",
    "    zero_counter = 0\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        #data = torch.Tensor(feature.canny(data.reshape(28, 28).numpy(), sigma=1.0).astype(float)).reshape(1,1,28,28)\n",
    "        #print(label)\n",
    "        #if np.asscalar(target.numpy()) == 0:\n",
    "        #   continue\n",
    "        #print(counter)\n",
    "        #target = torch.Tensor([0]*batch_size).type(torch.LongTensor)#.to(torch.LongTensor)\n",
    "        adv, batch_l2_norm = adversary(model, detectors, data, target, to_numpy=False)\n",
    "\n",
    "\n",
    "        #assert isinstance(adversarial_examples, torch.FloatTensor)\n",
    "        #assert adv.size() == inputs.size()\n",
    "        #print(success)\n",
    "        for i in range(batch_size):\n",
    "            #print(adv.size())\n",
    "            if batch_l2_norm[i] != np.inf:\n",
    "                #output = model(adv[i].reshape(1,1,28,28))\n",
    "                #pred = output.argmax(dim=1, keepdim=True)\n",
    "                #print(\"out \", pred)\n",
    "                #if np.asscalar(pred.numpy()) == np.asscalar(target.numpy()[i]):\n",
    "                #    print(\"appended\")\n",
    "                success += 1\n",
    "                adv_imgs.append(adv[i])\n",
    "                l2_norms.append(batch_l2_norm[i])\n",
    "        if counter == 10:\n",
    "            break\n",
    "        else\n",
    "            counter += 1\n",
    "    print(\"KD Untargeted: conf: \", conf)    \n",
    "    print(\"Success Rate: \", success/1000)\n",
    "    print(\"Avg l2 Norm: \", np.mean(l2_norms))\n",
    "    \n",
    "    with open(\"adv_imgs/cifar/cw_KD_untargeted_cifar_adv_imgs_conf_\"+str(conf)+\\\n",
    "              \"_test_loader_canny_sigma_1.8.pkl\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(adv_imgs, fp)\n",
    "    with open(\"adv_imgs/cifar/cw_KD_untargeted_cifar_l2_norms_conf_\"+\\\n",
    "              str(conf)+\"_test_loader_canny_sigma_1.8.pkl\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(adv_imgs, fp)        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
