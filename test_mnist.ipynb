{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms, utils\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import argparse\n",
    "from numpy import ma\n",
    "import scipy\n",
    "import random\n",
    "import time\n",
    "import os.path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "from skimage import feature\n",
    "\n",
    "from network.cnn_net import cnn_net\n",
    "from network.ann import ann_net\n",
    "from network.adv_cnn_net import adv_cnn_net\n",
    "from network.adv_ann import adv_ann_net\n",
    "import utils.cw_final as cw_final\n",
    "import utils.cw as cw\n",
    "\n",
    "\n",
    "def train_binary(args, model, device, train_loader, optimizer, epoch, target_class):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        size = target.numpy().shape[0]\n",
    "        \n",
    "        ## canny edge detection stub\n",
    "        #print(data.size())\n",
    "        for i in range(size):\n",
    "            data[i] = torch.Tensor(feature.canny(data[i].reshape(28, 28).numpy(), sigma=1.8).astype(float))\n",
    "        \n",
    "        data = data.reshape(-1, 784) # Remove this line while training cnn\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        #print(size)\n",
    "        index_lst = []\n",
    "        for i in range(size):\n",
    "            if target[i] != target_class:\n",
    "                #print(target[i])\n",
    "                target[i] = 1\n",
    "                #print(target[i])\n",
    "            else:\n",
    "                target[i] = 0\n",
    "                index_lst.append(i)\n",
    "        \n",
    "        # data repeatation\n",
    "        repeat_data = torch.zeros(len(index_lst)*13, 784)        \n",
    "        repeat_target = torch.zeros(len(index_lst)*13).type(torch.LongTensor)\n",
    "        ind = 0\n",
    "        for i in index_lst:\n",
    "            for j in range(13):\n",
    "                repeat_data[ind] = data[i]\n",
    "                ind += 1\n",
    "        #print(repeat_target.size())\n",
    "        #print(target.size())\n",
    "        data = torch.cat((data, repeat_data), 0)\n",
    "        target = torch.cat((target, repeat_target), 0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    \n",
    "def test_binary(args, model, device, test_loader, target_class):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            \n",
    "            #canny edge detection stub\n",
    "            for i in range(data.size(0)):\n",
    "                data[i] = torch.Tensor(feature.canny(data[i].reshape(28, 28).numpy(), sigma=1.8).astype(float))\n",
    "                if target[i].data != target_class:\n",
    "                    target[i] = torch.Tensor([1]).type(torch.LongTensor)#.to(torch.LongTensor)\n",
    "                else:\n",
    "                    target[i] = torch.Tensor([0]).type(torch.LongTensor)\n",
    "            \n",
    "            data = data.reshape(data.size(0), 784) # Remove this line while training cnn\n",
    "                \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            #print(output)\n",
    "            #test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1)#, keepdim=True)  # get the index of the max log-probability            \n",
    "            #print(pred[0])\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    \n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #data = data.reshape(-1, 784) # Remove this line while training cnn\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            #data = data.reshape(-1, 784) # Remove this line while training cnn\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def calc_confusion(data, target, adv_imgs, classifier, detectors):\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    true_neg = 0\n",
    "    false_neg = 0\n",
    "\n",
    "    for i in range(len(adv_imgs)):\n",
    "        adv_im = adv_imgs[i]\n",
    "        \n",
    "        # canny edge sdetection stub\n",
    "        \n",
    "        #sigma = 1.8 gives best results\n",
    "        \n",
    "        output = classifier(torch.Tensor(adv_im).reshape(1, 1, 28, 28))\n",
    "        #print(output)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        #print(pred)\n",
    "        adv_edge = feature.canny(adv_im, sigma=1.8).astype(float)\n",
    "        det_out = detectors[pred](torch.Tensor(adv_edge).reshape(1, 784))\n",
    "        prediction = det_out.argmax(dim=1, keepdim=True)\n",
    "        if prediction == 0:\n",
    "            false_neg += 1\n",
    "        else:\n",
    "            true_pos += 1\n",
    "\n",
    "    output = classifier(data)\n",
    "    pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "    for i in range(args.test_batch_size):\n",
    "        prediction = np.asscalar(pred[i].numpy())\n",
    "\n",
    "        if (prediction != np.asscalar(target[i].numpy())): #ignoring natural errors\n",
    "            continue\n",
    "            \n",
    "        edge_data = feature.canny(data[i].reshape(28, 28).numpy(), sigma=1.8).astype(float)\n",
    "        detectors[prediction].eval()\n",
    "        output = detectors[prediction](torch.Tensor(edge_data).reshape(1, 784))\n",
    "        prediction = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "        if prediction == 0:\n",
    "            true_neg += 1\n",
    "        else:\n",
    "            false_pos += 1\n",
    "    \n",
    "    conf_mat = [true_neg, false_neg, false_pos, true_pos]\n",
    "    \n",
    "    return conf_mat \n",
    "\n",
    "\n",
    "def get_data(data_loader, targeted):\n",
    "    data, target = next(iter(data_loader))\n",
    "    \n",
    "    if targeted:\n",
    "        for i in range(target.size(0)):\n",
    "            rand_target = random.randint(0, 9)\n",
    "            while rand_target == target[i].item():\n",
    "                rand_target = random.randint(0, 9)\n",
    "            target[i] = rand_target    \n",
    "    \n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing')\n",
    "parser.add_argument('--epochs', type=int, default=50, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=True,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                    help='For Saving the current Model')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "assert (10000 % args.test_batch_size) == 0 #necessary for correct calculation of confusion matrix\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/mnist_digit', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       #transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       transforms.Normalize((0,), (1,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/mnist_digit', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.1307,), (0.3081,))\n",
    "        transforms.Normalize((0,), (1,))\n",
    "    ])),\n",
    "    batch_size=args.test_batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "adv_detectors = []\n",
    "detectors = []\n",
    "for i in range(10):\n",
    "    detectors.append(ann_net().to(device))\n",
    "    adv_detectors.append(adv_ann_net().to(device))\n",
    "    detectors[i].load_state_dict(torch.load(\"models/mnist_ann_canny/\"\\\n",
    "                                            +\"mnist_binary/mnist_ann_canny_binary_sigma_1.8_\"\\\n",
    "                                            + str(i) + \".pth\"))\n",
    "    adv_detectors[i].load_state_dict(torch.load(\"models/mnist_ann_canny/\"\\\n",
    "                                                +\"mnist_binary/mnist_ann_canny_binary_sigma_1.8_\"\\\n",
    "                                                + str(i) + \".pth\"))\n",
    "    detectors[i].to(device)\n",
    "    adv_detectors[i].to(device)\n",
    "    detectors[i].eval()\n",
    "    adv_detectors[i].eval()\n",
    "\n",
    "INPUT_BOX = (0.0, 1.0)\n",
    "OPT_LR = 0.01\n",
    "SEARCH_STEPS = 9\n",
    "\n",
    "model = adv_cnn_net().to(device)\n",
    "model.load_state_dict(torch.load(\"mnist_cnn.pt\"))\n",
    "model.eval()\n",
    "\n",
    "classifier = cnn_net().to(device)\n",
    "classifier.load_state_dict(torch.load(\"mnist_cnn.pt\"))\n",
    "classifier.eval()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "    model = nn.DataParallel(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Mddel Accuracy\n",
    "model = cnn_net()#.to(device)\n",
    "#model.load_state_dict(torch.load(\"mnist_cnn.pt\"))\n",
    "#model.eval()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, train_loader, optimizer, epoch)\n",
    "    test(args, model, device, test_loader)\n",
    "    \n",
    "torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "print(\"Final Model Accuracy:\")\n",
    "test(args, model, device, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Model Accuracy:\n",
    "\n",
    "Test set: Average loss: 0.0359, Accuracy: 9878/10000 (99%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Classifiers training and accuracy\n",
    "\n",
    "model = ann_net().to(device)\n",
    "optimizer = None\n",
    "success = 0\n",
    "for m in range(10):\n",
    "    # for training of the cnn-network\n",
    "    #if (args.save_model):\n",
    "    \n",
    "    print(\"class: \", m)\n",
    "    model.state_dict(torch.load(\"models/mnist_ann_canny/mnist_ann_canny_binary_sigma_1.8_\"+ str(m) + \".pt\"))\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_binary(args, model, device, train_loader, optimizer, epoch, m)\n",
    "        test_binary(args, model, device, test_loader, m)\n",
    "        \n",
    "    print(\"Accruacy for binary Classifier \" + str(m), test_binary(args, model, device, test_loader, m))\n",
    "    torch.save(model.state_dict(), \"models/mnist_ann_canny/mnist_ann_canny_binary_sigma_1.8_\"+ str(m) + \".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accruacy for binary Classifier 0 98\n",
    "Accruacy for binary Classifier 1 100\n",
    "Accruacy for binary Classifier 2 97\n",
    "Accruacy for binary Classifier 3 97\n",
    "Accruacy for binary Classifier 4 97\n",
    "Accruacy for binary Classifier 5 96\n",
    "Accruacy for binary Classifier 6 98\n",
    "Accruacy for binary Classifier 7 98\n",
    "Accruacy for binary Classifier 8 95\n",
    "Accruacy for binary Classifier 9 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UD case: Untargeted\n",
    "#       1) create adv imgs for trained model without defence for different confidence values\n",
    "#       2) Mix with equal number of natural imgs\n",
    "#       3) Calculate accuracy with defence\n",
    "\n",
    "conf_list = [0, 10, 20]\n",
    "data, target = get_data(test_loader, False)\n",
    "data, target = data.to(device), target.to(device)\n",
    "\n",
    "for conf in conf_list:    \n",
    "    adversary = cw.L2Adversary(targeted=False,\n",
    "                                   confidence=conf,\n",
    "                                   search_steps=SEARCH_STEPS,\n",
    "                                   box=INPUT_BOX,\n",
    "                                   optimizer_lr=OPT_LR)\n",
    "    adv_imgs = []\n",
    "    adv, l2 = adversary(model, data, target, to_numpy=False)\n",
    "    for i in range(args.test_batch_size):\n",
    "        if l2[i] != np.inf:    #check if attack was successful\n",
    "            adv_imgs.append(adv[i].reshape(28,28).numpy())\n",
    "       \n",
    "    with open(\"adv_imgs/UD_untargeted_mnist_adv_imgs_cw_conf_\"+str(conf)+\".pkl\", \"wb\") as fp:\n",
    "        pickle.dump(adv_imgs, fp)            \n",
    "    \n",
    "    print(\"Confidence: \", conf)\n",
    "    print(\"Attack Success Rate: \", len(adv_imgs) / 1000)\n",
    "    print(\"AVG l2: \", np.mean(l2))\n",
    "    \n",
    "    #conf_mat => [true_neg, false_neg, false_pos, true_pos]\n",
    "    conf_mat = calc_confusion(data, target, adv_imgs, classifier, detectors)        \n",
    "    \n",
    "    print(\"True Positive: \", conf_mat[3])\n",
    "    print(\"False Positive: \", conf_mat[2])\n",
    "    print(\"True Negative: \", conf_mat[0])\n",
    "    print(\"False Negative: \", conf_mat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UD Case: Targeted Attack\n",
    "            \n",
    "conf_lst = [0, 10]\n",
    "data, target = get_data(test_loader, True)\n",
    "data, target = data.to(device), target.to(device)\n",
    "\n",
    "for conf in conf_lst:    \n",
    "    \n",
    "    adversary = cw.L2Adversary(targeted=True,\n",
    "                            confidence=conf,\n",
    "                            search_steps=SEARCH_STEPS,\n",
    "                            box=INPUT_BOX,\n",
    "                            optimizer_lr=OPT_LR)\n",
    "\n",
    "    adv_imgs = []\n",
    "    \n",
    "    adv, l2 = adversary(model, data, target, to_numpy=False)\n",
    "    for i in range(args.test_batch_size):\n",
    "        if l2[i] != np.inf:    #check if attack was successful\n",
    "            adv_imgs.append(adv[i].reshape(28,28).numpy())\n",
    "    with open(\"adv_imgs/UD_targeted_mnist_adv_imgs_cw_conf_\"+str(conf)+\".pkl\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(adv_imgs, fp)            \n",
    "    \n",
    "    print(\"Confidence: \", conf)\n",
    "    print(\"Attack Success Rate: \", len(adv_imgs) / 1000)\n",
    "    print(\"AVG l2: \", np.mean(l2))\n",
    "        \n",
    "    #conf_mat => [true_neg, false_neg, false_pos, true_pos]\n",
    "    conf_mat = calc_confusion(data, target, adv_imgs, classifier, detectors)        \n",
    "\n",
    "    print(\"True Positive: \", conf_mat[3])\n",
    "    print(\"False Positive: \", conf_mat[2])\n",
    "    print(\"True Negative: \", conf_mat[0])\n",
    "    print(\"False Negative: \", conf_mat[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KD Targeted\n",
    "            \n",
    "conf_list = [0, 10, 20]\n",
    "data, target = get_data(test_loader, True)\n",
    "data, target = data.to(device), target.to(device)\n",
    "\n",
    "for conf in conf_list:\n",
    "    adversary = cw_final.L2Adversary(targeted=True,\n",
    "                                   confidence=conf,\n",
    "                                   search_steps=SEARCH_STEPS,\n",
    "                                   box=INPUT_BOX,\n",
    "                                   optimizer_lr=OPT_LR)\n",
    "\n",
    "    adv_imgs = []\n",
    "    l2_norms = []\n",
    "    success = 0\n",
    "\n",
    "    adv, batch_l2_norm = adversary(model, adv_detectors, data, target, to_numpy=False)\n",
    "\n",
    "    for i in range(args.test_batch_size):\n",
    "        if batch_l2_norm[i] != np.inf: \n",
    "            success += 1\n",
    "            adv_imgs.append(adv[i].reshape(28,28).numpy())\n",
    "            l2_norms.append(batch_l2_norm[i])        \n",
    "                            \n",
    "print(\"success rate: \", success/(1000))\n",
    "print(\"mean l2 norm:\", np.mean(l2_norms))\n",
    "\n",
    "with open(\"adv_imgs/cw_mnist_adv_imgs_conf_\"+str(conf)+\"_test_loader_KD_targeted_sigma_1.8_\"+\\\n",
    "          str(target_class)+\".pkl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(adv_imgs, fp)\n",
    "with open(\"adv_imgs/cw_mnist_l2_norms_conf_\"+str(conf)+\"_test_loader_KD_targeted_sigma_1.8_\"+\\\n",
    "          str(target_class)+\".pkl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(adv_imgs, fp)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KD Case:\n",
    "# untargeted attack: success rate, avg l2 norms\n",
    "\n",
    "conf_list = [0,10, 20]\n",
    "data, target = get_data(test_loader, False)\n",
    "data, target = data.to(device), target.to(device)\n",
    "\n",
    "for conf in conf_list: \n",
    "    \n",
    "    adversary = cw_final.L2Adversary(targeted=False,\n",
    "                                     confidence=conf,\n",
    "                                     search_steps=SEARCH_STEP,\n",
    "                                     box=INPUT_BOX,\n",
    "                                     optimizer_lr=OPT_LR)\n",
    "\n",
    "    adv_imgs = []\n",
    "    l2_norms = []\n",
    "    success = 0\n",
    "\n",
    "    adv, batch_l2_norm = adversary(model, adv_detectors, data, target, to_numpy=False)\n",
    "\n",
    "    for i in range(args.test_batch_size):\n",
    "        if batch_l2_norm[i] != np.inf:\n",
    "            success += 1\n",
    "            adv_imgs.append(adv[i].reshape(28,28).numpy())\n",
    "            l2_norms.append(batch_l2_norm[i].numpy())\n",
    "\n",
    "\n",
    "    print(\"Success Rate: \", success/1000)\n",
    "    print(\"Avg l2 Norm: \", np.mean(l2_norms))\n",
    "    \n",
    "    with open(\"adv_imgs/cw_KD_untargeted_mnist_adv_imgs_conf_\"+str(conf)+\"_test_loader_canny_sigma_1.8.pkl\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(adv_imgs, fp)\n",
    "    with open(\"adv_imgs/cw_KD_untargeted_mnist_l2_norms_conf_\"+str(conf)+\"_test_loader_canny_sigma_1.8.pkl\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(adv_imgs, fp)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
